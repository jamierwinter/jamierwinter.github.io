[{"content":"","date":"2024-06-21T01:01:40+01:00","image":"https://jamierwinter.github.io/img/Rado_graph.png","permalink":"https://jamierwinter.github.io/post/the-rado-graph/","title":"An Introduction to the Rado Graph"},{"content":"Mathematics is unique as a subject, owing to the modularity of formal mathematical texts: corollaries follow theorems, which follows from lemmas and propositions, which in turn can be followed back to the original definitions underpinning their statements. With this structure in mind, it feels natural consider representing a piece of mathematical text as a graph, representing theorems, lemmas, propositions and corollaries as nodes, with a directed edge between two nodes $A$ and $B$ representing \u0026ldquo;$A$ is used to prove $B$\u0026rdquo;, a so-called \u0026ldquo;proof graph\u0026rdquo;. When the idea first came to me, I considered whether one could create one massive graph, representing the accumulation of all mathematical knowledge across all fields. However, I quickly came to realise that owing to the existence of various proofs of the same theorem, such a graph couldn\u0026rsquo;t possibly be well-defined. But applying this idea to some self-contained mathematical text, say, a textbook or a set of lecture notes should give us a well-defined graph worth studying.\nWhy study such a graph?\rSuch a graph allows us to capture the structure and dependencies of a mathematical text. One may identify important theorems, or, on the flip-side, unimportant ones. Indeed, algorithms such as PageRank exist which could be applied to this end. In addition, one may implement algorithms such as SimRank to assess the \u0026ldquo;similarity\u0026rdquo; between theorems. The existence of multiple paths between two nodes may signify inefficiencies (redundancies) in the text. Identifying these may allow an author to streamline their text, in turn helping those intent on learning from it to do this in a more efficient way.\nHow did I do this?\rIn order to do this, I used Memgraph lab, which can be downloaded here. I created\nCase Study: Algebra II: Groups and Rings\r","date":"2024-06-20T11:17:01+01:00","image":"https://jamierwinter.github.io/img/header_graphing_first_year_modules.jpg","permalink":"https://jamierwinter.github.io/post/graphing_first_year_modules/","title":"Analysing the Efficiency of First Year Maths Modules with Graphs"},{"content":" Welcome to my first blog post. I am an incoming 4th year MMath Mathematics student at the University of Warwick. As such, in my 2nd year I completed an essay, titled Hidden Markov Models and their Applications. To a large extent, this post will be a writeup of that essay.\n1: Introduction\rIn 1902, the Russian Orthodox mathematician Pavel Nekrasov used Bernoulli\u0026rsquo;s Weak Law of Large Numbers (WLLN) [1] to claim that humans have free will, drawing comparisons between expressions of free will and independent events within probability theory [2].\nThe Weak Law of Large Numbers: Suppose $X_1, X_2,\\dots$ are independent and identically distributed random variables with finite mean $\\mu$ and variance $\\sigma^2$, then the sample mean ${\\overline{X}_n}\\coloneqq \\frac{1}{n}(X_1 + \\dots + X_n)$ converges to $\\mu$ in probability as $n\\rightarrow\\infty$.\nNekrasov (incorrectly) claimed that independence was not just a sufficient condition for the WLLN to hold, but also a necessary one. That is, if a sequence of random variables were to conform to the WLLN, then the independence of these random variables would be guaranteed as a consequence. Since contemporary data representative of human decision-making, such as crime statistics, appeared to follow the WLLN [3], Nekrasov argued that human decision-making and actions must be independent, and thus in some sense expressions of free will.\nAndrey Markov, who was, importantly, secular, lambasted this claim, describing Nekrasov\u0026rsquo;s work as \u0026ldquo;an abuse of mathematics\u0026rdquo;. However, Markov\u0026rsquo;s grievances weren\u0026rsquo;t rooted in any ideological or theological differences, but rather Nekrasov\u0026rsquo;s assumption that independence was necessary for the WLLN to hold, which he set out to disprove by providing a counterexample; the result\u0026mdash; the eponymous Markov chain [4].\nWhat emerged as a mere counterexample to settle a debate in mathematical theology has since been rigorously studied and adapted, resulting in the development of a multitude of \u0026ldquo;Markov Models\u0026rdquo;, of which one of the most important is known as the hidden Markov model (HMM), which today has applications across data science, linguistics, epidemiology, finance, and physics. HMMs have also been a foundational and widely used technique in AI research for several decades, meaning that over 100 years since their conception, Markov chains still have theological relevance, as humanity looks to \u0026lsquo;play God\u0026rsquo; through the development of artificial intelligence.\nIn this essay, I will first define the concept of a Markov chain (assuming a brief familiarity with state transition diagrams) and develop the necessary theory for us to then adapt this into the hidden Markov model, which we will study for the remainder of the essay.\n2: Markov Chains\rFrom its origins in the WLLN, we know that a Markov chain is going to look like a sequence of random variables. We now formalise this notion. Throughout this section we will closely follow the definitions provided in ST202 Stochastic Processes [5] (a module offered by Warwick University\u0026rsquo;s stats department).\nStochastic Processes: A stochastic process is a family $\\{ X_i : i \\in \\mathcal{T} \\}$ of random variables, where $\\mathcal{T}$ is an indexing set. When $\\mathcal{T}$ has finite or countably infinite cardinality, we say ${X_i:i \\in \\mathcal{T}}$ is a \\emph{discrete-time stochastic process}.\nSince in this essay we will always use $\\mathbb{N}_0$ as our indexing set, we will from now denote a stochastic process by $(X_n)_{n \\geq 0}$. We will use $I$ to denote the state space for each $X_n$, and refer to a given $i \\in I$ as a state. By default these state spaces will be labelled ${1,2,\\dots m}$ in the case $\\lvert I \\rvert~=~m \\in \\mathbb{N}$ and ${1,2,\\dots}$ when $I$ is infinite. Finally, given a stochastic process $(X_n)_{n \\geq 0}$, $n \\in \\mathbb{N}$ and some states $i$ and $j$, we will denote the probability of the stochastic process moving from the state $i$ to $j$ by $P_{ij}$. That is, $P_{ij} \\coloneqq \\mathbb{P}(X_{n+1}=j \\mid X_n=i)$.\nThe Markov Property: A stochastic process $(X_n)_{n \\geq 0}$ is said to have the Markov property if $\\forall n \\in \\mathbb{N}$, and for any choice of states $i_0,\\dots,i_{n+1} \\in I$ we have $$\\mathbb{P}(X_{n+1}\\hspace{-0.1cm}=i_{n+1} \\mid X_n\\hspace{-0.1cm}=i_n,\\dots,X_1=i_1,X_0=i_0)=\\mathbb{P}(X_{n+1}=i_{n+1} \\mid X_n=i_n)$$ whenever the involved conditional probabilities are defined.\nMarkov Chains: A discrete-time stochastic process $(X_n)_{n \\geq 0}$ with state space $I$ is called a Markov chain if:\ni.) $(X_n)_{n \\geq 0}$ has the Markov property,\nii.) $\\sum_{j \\in I}P_{ij}=1$ $\\forall i \\in I$.\nWhen $\\lvert I \\rvert \u0026lt; \\lvert \\mathbb{N} \\rvert$ we refer to the Markov chain $(X_n)_{n \\geq 0}$ as \\emph{finite}.\nIntuitively, the definition of the Markov property tells us that, given a Markov chain $(X_n)_{n \\geq 0}$ in some state at time n (i.e. $X_n=i_n$), the next state the chain enters depends solely on $X_n$ rather than any previous states.\nExample: A Simple Weather Model\rWe consider a simple weather model for a town, where on any given day the weather is in one of three states: 1.) \u0026ldquo;Sunny\u0026rdquo;, 2.) \u0026ldquo;Cloudy\u0026rdquo;, 3.) \u0026ldquo;Rainy\u0026rdquo;. If the weather is sunny on a given day, it will be sunny the next with probability 0.6, cloudy with probability 0.3, or rainy with probability 0.1. Similarly, if it is cloudy on a given day, it will be sunny the next with probability 0.2, cloudy with probability 0.4, or rainy with probability 0.4. Finally, if it is rainy on a given day then it\u0026rsquo;ll be cloudy the next with probability 0.6 or rainy with probability 0.4.\nThis can be represented by a Markov chain because the probability of the weather moving from one state to another only depends on the current weather state (property i), and the sum of the probabilities of moving from a given state to each other state is 1 (property ii). These two properties are encoded within the definition of a state transition diagram, and so we have a correspondence between state transition diagrams and Markov chains.\nThe state transition diagram used above shows us the three possible states our Markov chain can be in, as well as the probabilities of moving between each of these states over a single time-step, which we refer to as \\emph{transition probabilities}. In this instance, we take the state space $I={1,2,3}$, corresponding to the \u0026ldquo;Sunny\u0026rdquo;, \u0026ldquo;Cloudy\u0026rdquo;, and \u0026ldquo;Rainy\u0026rdquo; states respectively.\nTransition Matrices: Given a Markov chain $(X_n)_{n \\geq 0}$ with finite state space $I$ such that $\\lvert I \\rvert = m$, we define its stochastic matrix or transition matrix as the $m\\times m$ matrix $P$ with $$ P \\coloneqq \\begin{pmatrix} P_{11} \u0026amp; P_{12} \u0026amp; \\cdots \u0026amp; P_{1m} \\\\ P_{21} \u0026amp; P_{22} \u0026amp; \\cdots \u0026amp; P_{2m} \\\\ \\vdots \u0026amp; \\vdots \u0026amp; \\ddots \u0026amp; \\vdots \\\\ P_{m1} \u0026amp; P_{m2} \u0026amp; \\cdots \u0026amp; P_{mm} \\end{pmatrix} $$\nwhere $P_{ij} = \\mathbb{P}(X_{m+1}=j \\mid X_m=i)$ as before.\nReferring back to the previous example, we can see that the stochastic matrix for the above state transition diagram is given by \\begin{equation*} P = \\begin{pmatrix} 0.6 \u0026amp; 0.3 \u0026amp; 0.1 \\\\ 0.2 \u0026amp; 0.4 \u0026amp; 0.4 \\\\ 0 \u0026amp; 0.6 \u0026amp; 0.4 \\end{pmatrix} \\end{equation*}\nThe Initial Distribution of a Markov Chain\rSo far we have related the state of a Markov chain a time $n$ to its state at time $n+1$, however right now we don\u0026rsquo;t have a good notion of what state the chain starts in. We introduce the notion of an \u0026ldquo;initial distribution\u0026rdquo; to address this.\nInitial distributions: The row vector $\\lambda=[\\lambda_1,\\dots,\\lambda_m]$ is called an initial distribution of a Markov chain $(X_n)_{n \\geq 0}$ with $\\lvert I \\rvert = m$ if:\ni.) $\\sum_{i=1}^m {\\lambda}_i=1$,\nii.) ${\\lambda}_i \\geq 0$ for all $i \\in I$,\niii.) $X_0 \\sim \\lambda$. i.e. $\\mathbb{P}(X_0=i)=\\lambda_i$ $\\forall i \\in I$\nFor example, consider the Markov chain $(X_n)_{n \\geq 0}$ with $I={1,2}$ and corresponding stochastic matrix $P$ and initial distribution $\\lambda$ given by \\begin{equation*} P = \\begin{pmatrix} 0 \u0026amp; 1 \\ 2/5 \u0026amp; 3/5 \\ \\end{pmatrix} \\hspace{1cm} \\lambda= \\begin{bmatrix} \\frac{2}{3} \u0026amp; \\frac{1}{3} \\end{bmatrix} \\end{equation*} Suppose we want to find $\\mathbb{P}(X_2=1)$. Noting that $\\mathbb{P}(X_0=1) = \\frac{2}{3}$ and $\\mathbb{P}(X_0=2) = \\frac{1}{3}$ from our initial distribution $\\lambda$, we proceed as follows, first applying the Law of Total Probability: \\begin{align*} \\mathbb{P}(X_2=1) \u0026amp;= \\mathbb{P}(X_2=1 \\mid X_1=1)\\mathbb{P}(X_1=1) + \\mathbb{P}(X_2=1 \\mid X_1=2)\\mathbb{P}(X_1=2)\\\\ \u0026amp;=P_{1,1}\\mathbb{P}(X_1=1)+ P_{2,1}\\mathbb{P}(X_1=2)\\\\ \u0026amp;=\\frac{2}{5}\\mathbb{P}(X_1=2)\\\\ \u0026amp;=\\frac{2}{5}(\\mathbb{P}(X_1\\hspace{-0.1cm}=2 \\mid X_0\\hspace{-0.1cm}= 1)\\mathbb{P}(X_0\\hspace{-0.1cm}=1) + \\mathbb{P}(X_1\\hspace{-0.1cm}=2 \\mid X_0\\hspace{-0.1cm}=2)\\mathbb{P}(X_0\\hspace{-0.1cm}=2))\\\\ \u0026amp;=\\frac{2}{5}(P_{1,2}\\mathbb{P}(X_0=1) + P_{2,2}\\mathbb{P}(X_0=2))\\\\ \u0026amp;=\\frac{2}{5}(\\frac{2}{3}\\cdot1+\\frac{3}{5}\\cdot\\frac{1}{3})\\\\ \u0026amp;=\\frac{26}{75} \\end{align*}\nThe Stationary Distribution of a Markov Chain\rlorem ipsum\nReferences\r[1] Krys Latuszynski, Alice Kirichenko, Anthony Lee, and Wilfrid S Kendall. ST220 Introduction to Mathematical Statistics, Lecture Notes, University of Warwick. 2022.\n[2] Pavel Nekrasov. The Philosophy and Logic of Science of Mass Phenom- ena in Human Activity. Moscow, 1902 (In Russian).\n[3] Brian Hayes. “First links in the Markov chain”. American Scientist, 101:92–96, 2013.\n[4] A. A. Markov. “Extension of the law of large numbers to dependent quantities”. Proceedings of the Physical and Mathematical Society of Kazan University, 15:135–156, 1906 (In Russian).\n[5] Matt Ball, Ed Jackson, and Rosalia Linfield. ST202 Stochastic Pro- cesses, Lecture Notes, University of Warwick. 2022.\n[6] Charles Grinstead and Laurie J Snell. Introduction to Probability. American Mathematical Society, 1997.\n[7] S. U. Pillai, T. Suel, and S. Cha. “The Perron-Frobenius theorem: Some of its applications”. IEEE Signal Processing Magazine, 22:63–64, 2005.\n[8] Lawrence R. Rabiner. “A tutorial on hidden Markov models and se- lected applications in speech recognition”. Proceedings of the IEEE, 77:257–286, 1989.\n[9] Leonard E. Baum. “An inequality and associated maximization tech- nique in statistical estimation for probabilistic functions of Markov pro- cesses”. Inequalities, 3:1–8, 1972.\n[10] Daniel Jurafsky and James H. Martin. “Hidden Markov Models”. Speech and Language Processing, pages 7–10, 2018.\n[11] Andrew Viterbi. “Error bounds for convolutional codes and an asymp- totically optimum decoding algorithm”. IEEE transactions on Infor- mation Theory, 13:260–269, 1967.\n[12] Bing Liu. Sentiment Analysis: Mining Opinions, Sentiments, and Emo- tions. Cambridge University Press, 2020.\n","date":"2024-06-18T22:47:07+01:00","image":"https://jamierwinter.github.io/img/HMM.png","permalink":"https://jamierwinter.github.io/post/hmmsandtheirapplications/","title":"Hidden Markov Models and their Applications"}]